"""
**Jackknife** errors for (non-linear) functions of uncertain data, i.e. g(<a>,<b>,...)

Inspired by https://github.com/ararslan/Jackknife.jl.
"""
module Jackknife



import Statistics: mean, var

export jackknife_full, jackknife


"""
    jackknife_full(g::Function, x::AbstractArray) -> je, jstd

Jackknife estimate `je` and standard error `jstd` of `g(<a>,<b>,...)`.

Columns of `x` are time series of random variables, i.e. `x[i,1] = a_i`
and `x[i,2] = b_i`. For a given matrix input `x` the function `g`
must produce a scalar, for example `g(x) = @views mean(x[:,1])^2 - mean(x[:,2].^2)`.
"""
function jackknife_full(g::Function, samples::AbstractVector{<:Number}...)
    reduced_results = leaveoneout(g, samples...)
    return estimate(g, samples...; reduced_results = reduced_results),
           bias(g, samples...; reduced_results = reduced_results),
           _std_error(reduced_results)
end
function jackknife_full(g::Function, samples::AbstractArray{<:Number})
    jackknife_full(g, [samples[:, i] for i in 1:size(samples, 2)]...)
end

"""
    jackknife(g::Function, x::AbstractArray) -> jstd

Jackknife standard error `jstd` of `g(<a>,<b>,...)`.

Columns of `x` are time series of random variables, i.e. `x[i,1] = a_i`
and `x[i,2] = b_i`. For a given matrix input `x` the function `g`
must produce a scalar, for example `g(x) = @views mean(x[:,1])^2 - mean(x[:,2].^2)`.
"""
function jackknife(g::Function, samples::AbstractVector{<:Number}...)
    reduced_results = leaveoneout(g, samples...)
    return _std_error(reduced_results)
end
function jackknife(g::Function, samples::AbstractArray{<:Number})
    jackknife(g, [samples[:, i] for i in 1:size(samples, 2)]...)
end


"""
    leaveoneout(g::Function, samples::AbstractVector{<:Number}...)

Generates N sub-samples for each sample in `samples`, where one value is left
out and applies each sample mean to the function `g`. The result is used for
a 1-jackknife.

## Example:

Let `x` be a sample of length N (e.g. a time series of N random variables
generated by a Monte-Carlo algorithm) and `g` be some function acting on the
mean of that sample, e.g.
`g(first_moment, second_moment) = second_moment - first_moment^2`.
`leaveoneout(g, x, x.^2)` will generate N sub-samples `y_n` and `y_n.^2`, each
missing one (different) value from `x` and `x.^2` respectively. It will then
calculate `g(mean(y_n), mean(y_n.^2))` for each sub-sample and return the
result as a vector.
"""
function leaveoneout(g::Function, samples::AbstractVector{<:Number}...)
    @assert(
        length(samples[1]) > 1,
        "The sample must have multiple values! (length(samples[1]) > 1)"
    )

    full_sample_sums = map(sum, samples)
    invN1 = 1 / (length(samples[1]) - 1)
    red_sample_means = map(samples, full_sample_sums) do sample, full_sample_sum
        [(full_sample_sum - sample[i]) * invN1 for i in eachindex(sample)]
    end
    g.(red_sample_means...)
end


"""
    var(g::Function, samples::AbstractVector...)

Compute the jackknife estimate of the variance of `g(<a>, <b>, ...)` for a set
of samples (time series) `[a_1, ..., a_N]`, `[b_1, ..., b_N]`, etc.

For more details, see also [`leaveoneout](@ref).
"""
function var(g::Function, samples::AbstractVector{<:Number}...)
    _var(leaveoneout(g, samples...))
end
_var(gis::AbstractVector{<:Complex}) = _var(real(gis)) + _var(imag(gis))
function _var(reduced_results::AbstractVector{<:Real})
    # With N values each sample, N subsamples are created and N reduced_results
    # are calculated.
    N = length(reduced_results)
    # Eq. (3.35) in QMC Methods book
    return var(reduced_results) * (N - 1)^2 / N
end


"""
    std_error(g::Function, x::AbstractArray)

Compute the jackknife estimate of the one sigma error of `g(<a>,<b>,...)`, where
`g` is given as a function that computes a point estimate (scalar) when passed a
matrix `x`. Columns of `x` are time series of the random variables.

For more details, see also [`leaveoneout](@ref).
"""
function std_error(g::Function, samples::AbstractVector{<:Number}...)
    _std_error(leaveoneout(g, samples...))
end
function _std_error(gis::AbstractVector{<:Complex})
    sqrt(_std_error(real(gis))^2 + _std_error(imag(gis))^2)
end
_std_error(gis::AbstractVector{<:Real}) = sqrt(_var(gis))


"""
    bias(g::Function, x::AbstractArray)

Compute the jackknife estimate of the bias of `g(<a>,<b>,...)`, which computes a
point estimate when passed a matrix `x`. Columns of `x` are time series of the
random variables.

For more details, see also [`leaveoneout](@ref).
"""
function bias(
        g::Function,
        samples::AbstractVector{<:Number}...;
        reduced_results::AbstractVector{<:Number} = leaveoneout(g, samples...)
    )
    # Basically Eq. (3.33)
    (length(reduced_results) - 1) * (
        mean(reduced_results) - g(map(mean, samples)...)
    )
end


"""
    estimate(g::Function, x)

Compute the bias-corrected jackknife estimate of `g(<a>,<b>,...)`, which
computes a point estimate when passed a matrix `x`. Columns of `x` are time
series of the random variables.

For more details, see also [`leaveoneout](@ref).
"""
function estimate(
        g::Function,
        samples::AbstractVector{<:Number}...;
        reduced_results::AbstractVector{<:Number} = leaveoneout(g, samples...)
    )
    # Eq. (3.34) in QMC Methods book
    n = length(reduced_results)
    return n * g(map(mean, samples)...) - (n - 1) * mean(reduced_results)
end



end # module


# TODO: Prebinning + Jackknife
